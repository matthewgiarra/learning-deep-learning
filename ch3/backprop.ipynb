{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ac1f30",
   "metadata": {},
   "source": [
    "# Backpropagation for multi-layer networks\n",
    "This exercise implements the backpropagation example in chapter 3, *Sigmoid Neurons and Backpropagation*. The figure shows a simple two-layer feed-forward neural network used to explain backpropagation. \n",
    "\n",
    "![](doc/neurons.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f656048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights for layer G\n",
    "w_xg0 = 0.3\n",
    "w_xg1 = 0.6\n",
    "w_xg2 = -0.1\n",
    "w_xg = np.array([w_xg0, w_xg1, w_xg2])\n",
    "\n",
    "# Weights for layer F\n",
    "w_gf0 = -0.2\n",
    "w_gf1 = 0.5\n",
    "w_gf = np.array([w_gf0, w_gf1])\n",
    "\n",
    "# Inputs\n",
    "x1 = -0.9\n",
    "x2 = 0.1\n",
    "x_xg = np.array([1.0, x1, x2]) # First element is 1 for bias. \n",
    "\n",
    "# True y\n",
    "y_true = 1.0\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.5 # Experiment with the learning rate\n",
    "\n",
    "# Epochs\n",
    "num_epochs = 500\n",
    "\n",
    "# Allocate space for the mean squared error\n",
    "mse = np.zeros(num_epochs)\n",
    "\n",
    "# Print update\n",
    "print(\"Starting weights: \")\n",
    "print(\"w_xg = \" + str(w_xg) + \", w_gf = \" + str(w_gf))\n",
    "print()\n",
    "for k in range(num_epochs):\n",
    "    \n",
    "    print(\"After epoch %d\" % k)\n",
    "    \n",
    "    ### Forward pass ###\n",
    "    \n",
    "    # Output of neuron G\n",
    "    z_g = np.dot(x_xg, w_xg)\n",
    "    y_g = np.tanh(z_g) # Save this for backprop\n",
    "    \n",
    "    # Output of neuron F\n",
    "    x_gf = np.array([1.0, y_g])\n",
    "    z_f = np.dot(x_gf, w_gf)\n",
    "    y_f = sigmoid(z_f) # Save this for backprop\n",
    "    \n",
    "    # Calculate error\n",
    "    MSE = 0.5 * np.power(y_f - y_true, 2) # Save this for backprop\n",
    "    mse[k] = MSE\n",
    "    print(\"y_true = %0.2f, y_pred = %0.2f (MSE = %0.2e)\" % (y_true, y_f, MSE))\n",
    "    print(\"Neuron G output y_g = %0.2f\" % y_g)\n",
    "    print(\"Neuron F output y_f = %0.2f\" % y_f)\n",
    "    \n",
    "    \n",
    "    ### Backward pass ###\n",
    "    \n",
    "    ## Backpropagate the errors to get the gradients\n",
    "    \n",
    "    #  Derivative of the error function\n",
    "    dE_df = y_f - y_true # d(network error)) / d(output of neuron F)\n",
    "    print(\"MSE' = %0.2e\" % dE_df)\n",
    "    \n",
    "    # Error for neuron F\n",
    "    df_dzf = y_f * (1.0 - y_f) # Derivative of the output -- S'(z_f) = d/d(z_f){S(z_f)} = S(z_f) * (1 - S(z_f))\n",
    "    err_F = dE_df * df_dzf\n",
    "    print(\"Error term F = %0.3f\" % err_F)\n",
    "    \n",
    "    # Error for neuron G\n",
    "    dzf_dg = w_gf1\n",
    "    dg_dzg = 1 - np.power(y_g, 2) # Derivative of the output of G: y_g = tanh(z_g); dy_g/dz_g = 1 - tanh^2(z_g) = 1 - y_g^2\n",
    "    err_G = err_F * dzf_dg * dg_dzg\n",
    "    print(\"Error term G = %0.4f\" % err_G)\n",
    "    \n",
    "    \n",
    "    ## Update the weights using the gradients\n",
    "    \n",
    "    # Calculate weight adjustments\n",
    "    delta_w_xg = -1 * learning_rate * x_xg * err_G \n",
    "    delta_w_gf = -1 * learning_rate * x_gf * err_F \n",
    "    print(\"Delta w_xg = \" + str(delta_w_xg))\n",
    "    print(\"Delta w_gf = \" + str(delta_w_gf))\n",
    "\n",
    "    # Update the weights\n",
    "    w_xg += delta_w_xg\n",
    "    w_gf += delta_w_gf\n",
    "    \n",
    "    # Print update\n",
    "    print(\"updated w_xg = \" + str(w_xg))\n",
    "    print(\"updated w_gf = \" + str(w_gf))\n",
    "    \n",
    "    print(\"EOL\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a42fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse, '-k')\n",
    "plt.plot(np.array([0, num_epochs]), np.array([0,0]), '--r')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean squared error\")\n",
    "plt.title(\"Training loss for simple neural network\")\n",
    "plt.grid(which=\"major\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
